{"cells":[{"cell_type":"markdown","metadata":{"id":"1Dk5VNYNt28C"},"source":["# Critérios para seleção de modelos"]},{"cell_type":"markdown","metadata":{"id":"HyJVGRwQt28F"},"source":["Na primeira aula enfatizamos a importância de saber escolher o modelo mais adequado para um cenário, mostrando a questão do equilíbrio viés-variância. Nesta aula vamos aprender alguns critérios para escolher o modelo mais adequado para o problema em questão.\n","\n","\n","- Complexidade do Modelo\n","- Interpretabilidade\n","- Eficiência Computacional\n","- Habilidade de Generalização\n","- Sensibilidade à Escala de Dados\n"]},{"cell_type":"markdown","metadata":{"id":"XsRZVRfct28F"},"source":["Vejamos cada critério, deixando claro que, por vezes, esses critérios interagem entre si."]},{"cell_type":"markdown","metadata":{"id":"TY9ckEYQt28G"},"source":["## Complexidade do modelo\n","\n","A complexidade do modelo refere-se ao número de parâmetros e à estrutura do modelo. Modelos mais complexos podem se ajustar melhor aos dados, mas correm o risco de overfitting.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"F10j59L6t28G"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>X</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.261961</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.903381</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.020782</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.899936</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.097239</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          X\n","0  0.261961\n","1  0.903381\n","2  0.020782\n","3  0.899936\n","4  0.097239"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","df = pd.read_csv('dados_trade_off_vies_variancia.csv')\n","\n","# criando X e y\n","X = df[['X']]\n","y = df[['y']]\n","\n","X.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qD1yI_K9t28H"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PydB0Z8Ft28H"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VmRTmw3At28I"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import numpy as np\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","\n","RANDOM_STATE = 1\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.30, random_state=RANDOM_STATE\n",")\n","\n","xfit = np.linspace(-0.1, 1.1, 1000)\n","df_xfit = pd.DataFrame({\"X\": xfit})\n","\n","ordens = (1, 3)\n","titulos = (\"Grau 1\", \"Grau 3\")\n","parametros_modelos = {}\n","\n","fig, axes = plt.subplots(1, 2, figsize=(15, 6), tight_layout=True)\n","\n","for ordem, titulo, ax in zip(ordens, titulos, axes):\n","    model = make_pipeline(PolynomialFeatures(ordem, include_bias=False), LinearRegression())\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(df_xfit)\n","\n","    parametros_modelos[titulo] = {\n","        \"coef\": model.named_steps[\"linearregression\"].coef_,\n","        \"intercept\": model.named_steps[\"linearregression\"].intercept_,\n","    }\n","\n","    ax.scatter(X_train, y_train, s=50, color=\"C0\", label=\"treino\")\n","    ax.scatter(X_test, y_test, s=50, color=\"C4\", label=\"teste\")\n","\n","    ax.plot(xfit, y_pred, color=\"C1\", label=\"predição\")\n","\n","    ax.set_title(titulo)\n","    ax.set_xlim(-0.1, 1.0)\n","    ax.set_ylim(-2, 13)\n","    ax.set_xlabel(\"X\")\n","    ax.set_ylabel(\"y\")\n","\n","    handles, labels = ax.get_legend_handles_labels()\n","\n","fig.suptitle(f\"Comparando ordens\\n\", fontsize=16)\n","fig.legend(\n","    loc=\"lower center\",\n","    ncol=3,\n","    bbox_to_anchor=(0.5, 0.875),\n","    bbox_transform=fig.transFigure,\n","    handles=handles,\n","    labels=labels,\n",")\n","\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMfANve3t28I"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qU0BOGSt28I"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"uGqNnpxbt28J"},"source":["Entendendo os coeficientes para o modelo de grau 1. Para modelos lineares, a equação geral do valor esperado para uma combinação linear de variáveis independentes (também chamadas de variáveis explicativas, preditoras ou regressoras) é dada por:\n","\n","$$y = w_0 + w_1x_1 + w_2x_2 + ... + w_p x_p$$\n","\n","Onde $w_0$ é o intercepto (o valor esperado de $y$ quando $x_1 = x_2 = ... = x_p = 0$) e $w_1, w_2, ..., w_p$ são os coeficientes das variáveis independentes.\n","\n","Para o modelo de grau 1, temos apenas uma variável independente, portanto, a equação geral pode ser escrita como:\n","\n","$$y = w_0 + w_1x_1$$\n","\n","Onde $w_0$ é o intercepto e $w_1$ é o coeficiente da variável independente $x_1$. O coeficiente $w_1$ representa a inclinação da reta de regressão, ou seja, a taxa de variação de $y$ em relação a $x_1$. Em outras palavras, se $x_1$ aumenta em uma unidade, $y$ aumenta em $w_1$ unidades. O intercepto $w_0$ representa o valor de $y$ quando $x_1$ é igual a zero. Em outras palavras, ele é o valor esperado de $y$ quando $x_1$ é igual a zero.\n"]},{"cell_type":"markdown","metadata":{"id":"CtE1rZ-Ht28J"},"source":["Assim, com base nos `coef` e `intercept` obtidos, a equação da reta é dada por:\n","\n","$$\n","y = 3.397 + 7.656 x_1\n","$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YZsdKxVst28J"},"source":["Para entender o polinômio de grau 3, vejamos como o array `X` fica após a transformação feita por `PolynomialFeatures`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H7zcPRtFt28J"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"pOGbLNDst28J"},"source":["Para um polinômio de grau *n*, a equação geral é dada por:\n","\n","$$y = w_0 + w_1x + w_2x^2 + ... + w_nx^n$$\n","\n","Onde $w_0$ é o termo de *bias* e $w_1, w_2, ..., w_n$ são os pesos associados a cada variável de entrada.\n","\n","Para um polinômio de grau 3, a equação geral é dada por:\n","\n","$$y = w_0 + w_1x + w_2x^2 + w_3x^3$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lt-ij-hSt28J"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"m6xTcMT6t28J"},"source":["Assim, nosso polinômio de grau 3 é:\n","\n","$$\n","y = 1.104 + 36.078x - 54.949x^2 + 26.338x^3\n","$$"]},{"cell_type":"markdown","metadata":{"id":"co5wYpZlt28J"},"source":["## Interpretabilidade\n","\n","A interpretabilidade refere-se à facilidade com que um modelo pode ser entendido e explicado. Modelos lineares, por exemplo, são mais fáceis de interpretar do que modelos de ordem superior, e estes são mais fáceis de interpretar do que modelos baseados em árvores ou redes neurais. A interpretabilidade é importante para a confiança em um modelo.\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
